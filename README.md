# Microservicio de SummarizaciÃ³n con LLM

Un microservicio backend robusto y escalable que recibe texto y devuelve resÃºmenes generados por OpenAI GPT-5 nano, priorizando latencia y confiabilidad.

## ğŸ¯ Objetivo

DiseÃ±ar un microservicio que genere resÃºmenes de texto usando OpenAI GPT-5 nano como proveedor principal, con un sistema de fallback extractivo usando TextRank para garantizar disponibilidad del servicio.

## ğŸ—ï¸ Arquitectura

```
Cliente â†’ API (FastAPI) â†’ OpenAI Provider
           â†³ Fallback: TextRank Extractivo
           â†³ Logs JSON Estructurados
           â†³ AutenticaciÃ³n API Key
```

### Componentes Principales

- **API FastAPI**: Valida, autentica y procesa requests
- **Proveedor LLM**: IntegraciÃ³n con OpenAI GPT-5 nano
- **Fallback Extractivo**: TextRank usando sumy para garantizar respuesta
- **Sistema de Logs**: Logs JSON estructurados para monitoreo
- **AutenticaciÃ³n**: API Key obligatoria para todos los endpoints

## ğŸš€ CaracterÃ­sticas

### âœ… Implementado (Fase Inicial)

- **Endpoint POST /v1/summarize**: Genera resÃºmenes con validaciÃ³n completa
- **Endpoint GET /v1/healthz**: Health check con verificaciÃ³n de conectividad
- **AutenticaciÃ³n API Key**: Sistema de seguridad con HTTPBearer
- **Logs JSON Estructurados**: Logging completo con request_id y mÃ©tricas
- **Fallback AutomÃ¡tico**: TextRank cuando OpenAI falla
- **Retries Inteligentes**: Exponential backoff para errores 429/5xx
- **Docker Compose**: Despliegue containerizado
- **Tests Comprehensivos**: Cobertura completa con pytest
- **DocumentaciÃ³n OpenAPI**: Interfaz interactiva en /docs

### ğŸ”„ Resiliencia y Confiabilidad

- **Timeouts Configurados**: Cliente 10s, LLM 8s
- **Retries AutomÃ¡ticos**: MÃ¡ximo 2 reintentos con exponential backoff
- **Fallback Garantizado**: TextRank asegura respuesta siempre
- **Health Check**: Monitoreo continuo del estado del servicio
- **Manejo Granular de Errores**: Logs detallados sin datos sensibles

## ğŸ“‹ Requisitos Previos

- Docker y Docker Compose
- Python 3.11+ (para desarrollo local)
- API Key de OpenAI vÃ¡lida

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd MacaqueTest
```

### 2. Configurar Variables de Entorno

```bash
cp .env.example .env
```

Editar `.env` con tus valores:

```env
# API Keys permitidas (separadas por coma)
API_KEYS_ALLOWED=tu-api-key-123,otra-api-key-456

# ConfiguraciÃ³n OpenAI
OPENAI_API_KEY=sk-tu-openai-api-key-aqui
OPENAI_MODEL=gpt-5-nano

# ConfiguraciÃ³n opcional
SUMMARY_MAX_TOKENS=500
LANG_DEFAULT=auto
LOG_LEVEL=INFO
```

### 3. Ejecutar con Docker Compose

```bash
# Construir y ejecutar
docker-compose up --build

# En modo detached
docker-compose up -d --build
```

### 4. Verificar InstalaciÃ³n

```bash
# Health check
curl http://localhost:8000/v1/healthz

# DocumentaciÃ³n interactiva
open http://localhost:8000/docs
```

## ğŸ”§ Desarrollo Local

### Instalar Dependencias

```bash
pip install -r requirements.txt
```

### Ejecutar Servicio

```bash
# Modo desarrollo
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Modo producciÃ³n
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Ejecutar Tests

```bash
# Todos los tests
pytest

# Tests con cobertura
pytest --cov=app

# Tests especÃ­ficos
pytest tests/test_summarize.py -v
```

## ğŸ“– Uso de la API

### AutenticaciÃ³n

Todos los endpoints requieren autenticaciÃ³n con API Key:

```bash
Authorization: Bearer tu-api-key-123
```

### Generar Resumen

```bash
curl -X POST "http://localhost:8000/v1/summarize" \
  -H "Authorization: Bearer tu-api-key-123" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Este es un texto largo que serÃ¡ resumido por el microservicio...",
    "lang": "es",
    "max_tokens": 150,
    "tone": "concise"
  }'
```

**Response:**
```json
{
  "summary": "Resumen generado del texto...",
  "usage": {
    "prompt_tokens": 120,
    "completion_tokens": 40,
    "total_tokens": 160
  },
  "model": "gpt-5-nano",
  "latency_ms": 1250,
  "fallback_used": false
}
```

### Health Check

```bash
curl http://localhost:8000/v1/healthz
```

**Response:**
```json
{
  "status": "ok",
  "timestamp": "2024-01-15T10:30:00.000Z",
  "latency_ms": 45,
  "checks": {
    "api": "ok",
    "llm_provider": "ok"
  }
}
```

## ğŸ Ejemplo con Python

```python
import httpx
import asyncio

async def summarize_text():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/v1/summarize",
            headers={"Authorization": "Bearer tu-api-key-123"},
            json={
                "text": "Texto a resumir...",
                "lang": "es",
                "max_tokens": 100,
                "tone": "neutral"
            }
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Resumen: {data['summary']}")
            print(f"Modelo: {data['model']}")
            print(f"Latencia: {data['latency_ms']}ms")
        else:
            print(f"Error: {response.status_code}")

# Ejecutar
asyncio.run(summarize_text())
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de Entorno

| Variable | DescripciÃ³n | Valor por Defecto |
|----------|-------------|-------------------|
| `API_KEYS_ALLOWED` | API Keys permitidas (separadas por coma) | **Requerido** |
| `OPENAI_API_KEY` | API Key de OpenAI | **Requerido** |
| `OPENAI_MODEL` | Modelo de OpenAI | `gpt-5-nano` |
| `SUMMARY_MAX_TOKENS` | MÃ¡ximo tokens para resumen | `500` |
| `LANG_DEFAULT` | Idioma por defecto | `auto` |
| `REQUEST_TIMEOUT_MS` | Timeout del request | `10000` |
| `LLM_TIMEOUT_MS` | Timeout de llamada LLM | `8000` |
| `MAX_RETRIES` | MÃ¡ximo nÃºmero de reintentos | `2` |
| `MAX_TEXT_LENGTH` | Longitud mÃ¡xima del texto | `50000` |
| `LOG_LEVEL` | Nivel de logging | `INFO` |

### Idiomas Soportados

- `auto`: DetecciÃ³n automÃ¡tica
- `es`: EspaÃ±ol
- `en`: InglÃ©s
- `fr`: FrancÃ©s
- `de`: AlemÃ¡n
- `it`: Italiano
- `pt`: PortuguÃ©s

### Tonos de Resumen

- `neutral`: Tono neutral y objetivo
- `concise`: Tono conciso y directo
- `bullet`: Formato de viÃ±etas

## ğŸ“Š Monitoreo y Logs

### Logs Estructurados JSON

El servicio genera logs JSON estructurados con:

```json
{
  "timestamp": "2024-01-15T10:30:00.000Z",
  "level": "INFO",
  "logger": "app.api.v1.endpoints.summarize",
  "message": "Request completado: /v1/summarize",
  "request_id": "123e4567-e89b-12d3-a456-426614174000",
  "endpoint": "/v1/summarize",
  "latency_ms": 1250,
  "model_used": "gpt-4o-mini",
  "tokens_used": {"prompt_tokens": 120, "completion_tokens": 40},
  "fallback_used": false
}
```

### MÃ©tricas de Rendimiento

- **Latencia**: Tiempo total de procesamiento
- **Tokens**: Uso de tokens de OpenAI
- **Fallback Rate**: Porcentaje de uso del fallback
- **Error Rate**: Tasa de errores por endpoint

## ğŸ§ª Testing

### Estructura de Tests

```
tests/
â”œâ”€â”€ conftest.py          # ConfiguraciÃ³n y fixtures
â”œâ”€â”€ test_auth.py         # Tests de autenticaciÃ³n
â”œâ”€â”€ test_summarize.py    # Tests del endpoint principal
â”œâ”€â”€ test_health.py        # Tests de health check
â””â”€â”€ test_fallback.py     # Tests del servicio de fallback
```

### Ejecutar Tests

```bash
# Todos los tests
pytest

# Tests especÃ­ficos
pytest tests/test_summarize.py -v

# Tests con cobertura
pytest --cov=app --cov-report=html

# Tests en paralelo
pytest -n auto
```

## ğŸ—ï¸ Estructura del Proyecto

```
MacaqueTest/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚   â”‚       â”œâ”€â”€ summarize.py   # POST /v1/summarize
â”‚   â”‚   â”‚       â””â”€â”€ health.py      # GET /v1/healthz
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py              # ConfiguraciÃ³n (12-factor)
â”‚   â”‚   â”œâ”€â”€ logging.py             # Logs estructurados JSON
â”‚   â”‚   â”œâ”€â”€ security.py            # AutenticaciÃ³n API Key
â”‚   â”‚   â””â”€â”€ middleware.py          # Middleware personalizado
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ requests.py            # Schemas Pydantic
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ llm_provider.py        # IntegraciÃ³n OpenAI
â”‚       â””â”€â”€ fallback.py            # Resumen extractivo TextRank
â”œâ”€â”€ tests/                         # Tests comprehensivos
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ”’ Seguridad

- **API Key Obligatoria**: Todos los endpoints requieren autenticaciÃ³n
- **ValidaciÃ³n Estricta**: ValidaciÃ³n completa de entrada
- **Logs Seguros**: No se loguean datos sensibles
- **Headers de Seguridad**: CORS, XSS protection, etc.
- **LÃ­mites de Payload**: MÃ¡ximo 50k caracteres de texto

## ğŸš€ Despliegue

### Docker Compose (Recomendado)

```bash
docker-compose up -d --build
```

### Docker Manual

```bash
# Construir imagen
docker build -t summarization-service .

# Ejecutar contenedor
docker run -p 8000:8000 --env-file .env summarization-service
```

### Variables de Entorno de ProducciÃ³n

```env
# ProducciÃ³n
LOG_LEVEL=WARNING
CORS_ORIGINS=https://tu-dominio.com
REQUEST_TIMEOUT_MS=15000
LLM_TIMEOUT_MS=10000
```

## ğŸ“ˆ Roadmap (Futuro)

### Fase 2 - CaracterÃ­sticas Opcionales

- [ ] **Redis Integration**: CachÃ© y rate limiting
- [ ] **Evaluador de Calidad**: MÃ©tricas automÃ¡ticas de resÃºmenes
- [ ] **MÃºltiples Proveedores**: Soporte para Anthropic, Cohere, etc.
- [ ] **MÃ©tricas Avanzadas**: Prometheus/Grafana
- [ ] **Rate Limiting**: LÃ­mites por API key
- [ ] **Batch Processing**: Procesamiento en lote

### Fase 3 - Escalabilidad

- [ ] **Load Balancing**: MÃºltiples instancias
- [ ] **Database Integration**: Persistencia de mÃ©tricas
- [ ] **Queue System**: Procesamiento asÃ­ncrono
- [ ] **Monitoring**: Alertas y dashboards

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crear branch para feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push al branch (`git push origin feature/nueva-funcionalidad`)
5. Abrir Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

- **DocumentaciÃ³n**: `/docs` en el servidor
- **Issues**: GitHub Issues
- **Logs**: Verificar logs del contenedor con `docker-compose logs -f`

---

**Desarrollado con â¤ï¸ usando FastAPI, OpenAI y Python**
