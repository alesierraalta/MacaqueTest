# Microservicio de Summarizaci√≥n con LLM

Un microservicio backend que recibe texto y devuelve res√∫menes generados por OpenAI GPT-5 nano, priorizando **latencia** y **confiabilidad** con fallback autom√°tico.

## üéØ Objetivo

Dise√±ar un microservicio backend que reciba texto y devuelva un resumen generado por un modelo de lenguaje (LLM), priorizando latencia y confiabilidad.

## üìã Requisitos Funcionales

### POST /v1/summarize
**Request:**
```json
{
  "text": "string",
  "lang": "auto|es|en|...",
  "max_tokens": 100,
  "tone": "neutral|concise|bullet"
}
```

**Response:**
```json
{
  "summary": "string",
  "usage": {
    "prompt_tokens": 120,
    "completion_tokens": 40
  },
  "model": "string",
  "latency_ms": 900,
  "fallback_used": false,
  "cached": false
}
```

### GET /v1/healthz
Revisi√≥n del estado del servicio y conectividad al LLM. Redis opcional.

**Auth:** API Key (`Authorization: Bearer <key>`)

## üöÄ Instalaci√≥n y Configuraci√≥n

### Paso 1: Clonar el Repositorio

```bash
git clone  https://github.com/alesierraalta/MacaqueTest.git
cd MacaqueTest
```

### Paso 2: Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar configuraci√≥n
nano .env
```

**Configuraci√≥n m√≠nima requerida:**
```env
# API Keys permitidas (separadas por coma)
API_KEYS_ALLOWED=tu-api-key-123,otra-api-key-456

# API Key de OpenAI (REQUERIDA)
OPENAI_API_KEY=sk-tu-openai-api-key-aqui

# Modelo de OpenAI
OPENAI_MODEL=gpt-5-nano

# Configuraci√≥n Redis (opcional)
ENABLE_REDIS=true
REDIS_URL=redis://redis:6379/0
CACHE_TTL_SECONDS=3600
RATE_LIMIT_REQUESTS=100
```

### Paso 3: Instalar Dependencias

```bash
# Instalar dependencias Python
pip install -r requirements.txt
```

### Paso 4: Ejecutar con Docker Compose (Recomendado)

```bash
# Construir y ejecutar servicios
docker-compose up --build

# En segundo plano
docker-compose up -d --build
```

### Paso 5: Verificar Instalaci√≥n

```bash
# Health check
curl http://localhost:8000/v1/healthz

# Respuesta esperada:
{
  "status": "ok",
  "timestamp": "2024-01-15T10:30:00.000Z",
  "latency_ms": 45,
  "checks": {
    "api": "ok",
    "llm_provider": "ok",
    "redis": "ok"
  }
}
```

## üìñ Uso de la API

### Documentaci√≥n Interactiva

```bash
# Abrir documentaci√≥n OpenAPI
open http://localhost:8000/docs
```

### Ejemplo 1: Resumen B√°sico

```bash
curl -X POST "http://localhost:8000/v1/summarize" \
  -H "Authorization: Bearer tu-api-key-123" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Buenos d√≠as Alejandro, un placer saludarte. Te contacto para decir que hemos recibido tu CV y nos gustar√≠a tener una charla para conocerte m√°s en profundidad. ¬øNos puedes facilitar huecos que tengas disponibles? Quedamos a la espera.",
    "lang": "es",
    "max_tokens": 150,
    "tone": "concise"
  }'
```

**Respuesta esperada:**
```json
{
  "summary": "Empresa interesada en CV de Alejandro solicita entrevista para conocerlo mejor y pide disponibilidad horaria.",
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0
  },
  "model": "gpt-5-nano",
  "latency_ms": 1250,
  "fallback_used": false,
  "cached": false
}
```

### Ejemplo 2: Resumen con Tono Bullet

```bash
curl -X POST "http://localhost:8000/v1/summarize" \
  -H "Authorization: Bearer tu-api-key-123" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "La empresa ABC ha reportado un aumento del 15% en sus ventas durante el √∫ltimo trimestre. Los productos m√°s vendidos fueron smartphones y laptops. El CEO mencion√≥ que planean expandirse a nuevos mercados en 2024.",
    "lang": "es",
    "max_tokens": 100,
    "tone": "bullet"
  }'
```

### Ejemplo 3: Resumen en Ingl√©s

```bash
curl -X POST "http://localhost:8000/v1/summarize" \
  -H "Authorization: Bearer tu-api-key-123" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "The quarterly earnings report shows significant growth in revenue. Our team has successfully launched three new products this quarter. Customer satisfaction ratings have improved by 20% compared to last quarter.",
    "lang": "en",
    "max_tokens": 80,
    "tone": "neutral"
  }'
```

### Ejemplo 4: Probar Cach√© de Redis

```bash
# Primera request (genera y cachea)
curl -X POST "http://localhost:8000/v1/summarize" \
  -H "Authorization: Bearer tu-api-key-123" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Este es un texto de prueba para verificar el cach√© de Redis",
    "lang": "es",
    "max_tokens": 50,
    "tone": "concise"
  }'
```

**Respuesta esperada (primera request):**
```json
{
  "summary": "Texto de prueba para verificar el cach√© de Redis",
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0
  },
  "model": "gpt-5-nano",
  "latency_ms": 1250,
  "fallback_used": false,
  "cached": false
}
```

```bash
# Segunda request (mismo texto - usa cach√©)
curl -X POST "http://localhost:8000/v1/summarize" \
  -H "Authorization: Bearer tu-api-key-123" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Este es un texto de prueba para verificar el cach√© de Redis",
    "lang": "es",
    "max_tokens": 50,
    "tone": "concise"
  }'
```

**Respuesta esperada (segunda request):**
```json
{
  "summary": "Texto de prueba para verificar el cach√© de Redis",
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 0,
    "total_tokens": 0
  },
  "model": "gpt-5-nano",
  "latency_ms": 0,
  "fallback_used": false,
  "cached": true
}
```

## üêç Ejemplo con Python

```python
import httpx
import asyncio

async def summarize_text():
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://localhost:8000/v1/summarize",
            headers={"Authorization": "Bearer tu-api-key-123"},
            json={
                "text": "Texto a resumir aqu√≠...",
                "lang": "es",
                "max_tokens": 100,
                "tone": "neutral"
            }
        )
        
        if response.status_code == 200:
            data = response.json()
            print(f"Resumen: {data['summary']}")
            print(f"Modelo: {data['model']}")
            print(f"Latencia: {data['latency_ms']}ms")
            print(f"Cacheado: {data['cached']}")
        else:
            print(f"Error: {response.status_code}")

asyncio.run(summarize_text())
```

## ‚öôÔ∏è Configuraci√≥n Completa

### Variables de Entorno (12-factor)

| Variable | Descripci√≥n | Valor por Defecto | Requerido |
|----------|-------------|-------------------|-----------|
| `API_KEYS_ALLOWED` | API Keys permitidas (separadas por coma) | - | ‚úÖ |
| `OPENAI_API_KEY` | API Key de OpenAI | - | ‚úÖ |
| `OPENAI_MODEL` | Modelo de OpenAI | `gpt-5-nano` | ‚ùå |
| `SUMMARY_MAX_TOKENS` | M√°ximo tokens para resumen | `500` | ‚ùå |
| `LANG_DEFAULT` | Idioma por defecto | `auto` | ‚ùå |
| `REQUEST_TIMEOUT_MS` | Timeout del request | `10000` | ‚ùå |
| `LLM_TIMEOUT_MS` | Timeout de llamada LLM | `8000` | ‚ùå |
| `MAX_RETRIES` | M√°ximo n√∫mero de reintentos | `2` | ‚ùå |
| `MAX_TEXT_LENGTH` | Longitud m√°xima del texto | `50000` | ‚ùå |
| `LOG_LEVEL` | Nivel de logging | `INFO` | ‚ùå |
| `ENABLE_REDIS` | Habilitar Redis | `true` | ‚ùå |
| `REDIS_URL` | URL de conexi√≥n Redis | `redis://redis:6379/0` | ‚ùå |
| `CACHE_TTL_SECONDS` | TTL del cach√© en segundos | `3600` | ‚ùå |
| `RATE_LIMIT_REQUESTS` | Requests por minuto por API key | `100` | ‚ùå |

### Tonos de Resumen Disponibles

- `neutral`: Tono neutral y objetivo
- `concise`: Tono conciso y directo  
- `bullet`: Formato de vi√±etas


## üîß Desarrollo Local

### Ejecutar sin Docker

```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar en modo desarrollo
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Ejecutar en modo producci√≥n
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

### Ejecutar Tests

```bash
# Todos los tests
pytest

# Tests con cobertura
pytest --cov=app

# Tests espec√≠ficos
pytest tests/test_summarize.py -v
pytest tests/test_redis.py -v
pytest tests/test_fallback.py -v
```

## üèóÔ∏è Arquitectura

```
Cliente ‚Üí API (FastAPI) ‚Üí LLM Provider
           ‚Ü≥ Fallback: resumen extractivo
           ‚Ü≥ Redis: cach√© + rate limiting
           ‚Ü≥ Logs JSON estructurados
```

### Componentes

- **API**: Valida, autentica y llama al LLM
- **Proveedor LLM**: OpenAI GPT-5 nano configurable
- **Fallback**: TextRank si el modelo falla
- **Redis**: Cach√© y rate limiting (opcional)

## üîí Seguridad

### Autenticaci√≥n Obligatoria

```bash
# Todos los endpoints requieren API Key
Authorization: Bearer tu-api-key-123
```

### Validaciones

- ‚úÖ Texto ‚â§ 50k caracteres
- ‚úÖ Idioma v√°lido
- ‚úÖ Entrada limpia
- ‚úÖ API Key obligatoria
- ‚úÖ Rate limit opcional (Redis)

### Logs Seguros

```json
{
  "timestamp": "2024-01-15T10:30:00.000Z",
  "level": "INFO",
  "logger": "app.api.v1.endpoints.summarize",
  "message": "Request completado: /v1/summarize",
  "request_id": "123e4567-e89b-12d3-a456-426614174000",
  "endpoint": "/v1/summarize",
  "latency_ms": 1250,
  "model_used": "gpt-5-nano",
  "tokens_used": {"prompt_tokens": 0, "completion_tokens": 0},
  "fallback_used": false,
  "cached": false
}
```

## üöÄ Escalabilidad y Resiliencia

### Timeouts Configurados

- **Cliente**: 10s (`REQUEST_TIMEOUT_MS`)
- **LLM**: 8s (`LLM_TIMEOUT_MS`)

### Retries Autom√°ticos

- Hasta 2 reintentos en errores 429/5xx
- Fallback extractivo ante fallas

### Redis (Opcional)

- **Cach√©**: Reduce latencia en textos repetidos
- **Rate Limiting**: 100 requests/minuto por API key
- **Graceful Degradation**: Servicio funciona si Redis falla

## üìä Monitoreo

### Health Check

```bash
curl http://localhost:8000/v1/healthz
```

**Estados posibles:**
- `ok`: Todo funcionando
- `degraded`: LLM fall√≥, fallback disponible
- `error`: Servicio no disponible

### M√©tricas Disponibles

- **Latencia**: Tiempo total de procesamiento
- **Modelo**: Modelo utilizado (gpt-5-nano o fallback)
- **Fallback Rate**: Uso del sistema de fallback
- **Cache Hit Rate**: Efectividad del cach√©
- **Error Rate**: Tasa de errores por endpoint

## üê≥ Despliegue

### Docker Compose (Recomendado)

```bash
# Construir y ejecutar
docker-compose up -d --build

# Ver logs
docker-compose logs -f

# Parar servicios
docker-compose down
```

### Docker Manual

```bash
# Construir imagen
docker build -t summarization-service .

# Ejecutar contenedor
docker run -p 8000:8000 --env-file .env summarization-service
```

### Variables de Producci√≥n

```env
LOG_LEVEL=WARNING
REQUEST_TIMEOUT_MS=15000
LLM_TIMEOUT_MS=10000
ENABLE_REDIS=true
RATE_LIMIT_REQUESTS=200
```

## üìÅ Estructura del Proyecto

```
MacaqueTest/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # Aplicaci√≥n FastAPI principal
‚îÇ   ‚îú‚îÄ‚îÄ api/v1/endpoints/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summarize.py           # POST /v1/summarize
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ health.py              # GET /v1/healthz
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuraci√≥n 12-factor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py             # Logs JSON estructurados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py            # Autenticaci√≥n API Key
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware.py          # Middleware personalizado
‚îÇ   ‚îú‚îÄ‚îÄ models/requests.py          # Schemas Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ llm_provider.py        # Integraci√≥n OpenAI GPT-5
‚îÇ       ‚îú‚îÄ‚îÄ fallback.py            # Resumen extractivo TextRank
‚îÇ       ‚îî‚îÄ‚îÄ redis_service.py       # Cach√© y rate limiting
‚îú‚îÄ‚îÄ tests/                         # Tests completos
‚îú‚îÄ‚îÄ docker-compose.yml            # Servicios Docker
‚îú‚îÄ‚îÄ Dockerfile                    # Imagen Docker
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias Python
‚îú‚îÄ‚îÄ .env                          # Variables de entorno
‚îú‚îÄ‚îÄ .gitignore                    # Archivos ignorados
‚îî‚îÄ‚îÄ README.md                     # Este archivo
```

## ‚úÖ Cumplimiento de Objetivos

### Latencia Optimizada
- ‚úÖ Cach√© Redis reduce latencia a 0ms en textos repetidos
- ‚úÖ Timeouts configurados (cliente 10s, LLM 8s)
- ‚úÖ Fallback r√°pido con TextRank

### Confiabilidad Garantizada
- ‚úÖ Fallback autom√°tico si LLM falla
- ‚úÖ Retries en errores 429/5xx
- ‚úÖ Graceful degradation si Redis falla
- ‚úÖ Health check completo

### Escalabilidad
- ‚úÖ Rate limiting por API key
- ‚úÖ Cach√© distribuido con Redis
- ‚úÖ Logs estructurados para monitoreo
- ‚úÖ Configuraci√≥n 12-factor

---

**Documentaci√≥n OpenAPI**: http://localhost:8000/docs  
**Health Check**: http://localhost:8000/v1/healthz